\chapter{Narrative Authoring Tools}\label{sec-tool}

As summarized at the end of Chapter 3, by learning from strategies used in practice and reflecting on the problems when crafting stories, the research community has devoted great efforts to building tools that help users to easily visualize data and communicate narratives. However, with the proliferation of narrative authoring tools, it arouses another challenge to evaluate and compare the strengths and weakness among these tools. A feasible approach is to adjust the different evaluation standards from the perspective of the research contribution. Therefore, in this chapter, we review the related narrative authoring tools with regards to their contributions, evaluation criteria, and evaluation methods in detail, and discuss the research opportunities concerning different parts.

\section{Approaches}

Different approaches feature prominently in tools intended to craft data-driven narratives. The approach selection is closely related to the contribution of the tool. In the following, we give an overview of existing tools that facilitate storytelling.

\subsection{Automatic Generation}
Automatic tools can be found in the applications of annotations and presentation orders. Contextifier \cite{Hullman2013} and NewsViews \cite{Gao2014} are both used to construct annotated visualization in news to contextualize text articles. Some works \cite{Hullman2013a, Kim2017a} leverage the directed graph model to automatically reason visualization sequencing in a linear order. However, no existing tool could craft a rich data story automatically. Most of them augment the storytelling in a specific element to some extent. This form provides hints for tool builders to determine which element in data-driven narratives could be improved by automatic generation and which asks for human-in-the-loop tools. 

\subsection{Authoring tools}

There already exists a multitude of general-purpose tools to create data graphics or videos, such as Adobe Creative Suite, Microsoft Excel and Power BI. However, most of them have a steep learning curve and are difficult for users to manipulate data directly. Thus, crafting data-driven narratives is much time-consuming with these tools. 

Recent narrative authoring tools \cite{Amini2017, Satyanarayan2014} are more expressive and efficient with a curated collection of visual design choices. For example, Satyanarayan and Heer \cite{Satyanarayan2014} instantiated a storytelling model in Ellipsis, which provides an interface for users to add elements to augment data storytelling in visualization. Amini et al. \cite{Amini2017} implemented DataClips, an authoring tool to craft a complete data video with a library of data-driven clips. These clips contain popular visualization types with corresponding animation types and can be easily extended to new clips. Some dedicated tools are built for specific storytelling elements. For instance, ChartAccent \cite{Ren2017} aims to support data-driven annotation. InfoNice \cite{Wang2018} enables easy creation of data-driven infographics. A list of narrative authoring tools with their major functions is summarized in Table \ref{tab:authoring-tool}. However, the proliferation of narrative authoring tools brings about difficulties and challenges for users to select the appropriate tool and evaluate its effectiveness. 

\subsection{Programming}

It is certainly possible to craft narrative visualization by programming. Actually, many computer science researchers work on this way and also provide a large palette of open-source libraries and packages, such as D3.js, processing, and Vega. Notably, there even exist libraries specific to some function, e.g., swoopyDrag.js or labella.js for data-driven annotation and labelling, and anime.js for animation. Iterative design between creating visualization with D3 and editing it with Illustrator is doable as well with the Hanpuku bridge tool \cite{Bigelow2017}. However, although these programming languages lend much freedom to customize data-driven narratives precisely, many visualization practitioners like data journalists are not equipped with capable programming skills. They require an interactive interface to author the story directly and intuitively. Therefore, many research efforts concentrate on developing better tools to ease the creation.

\section{Evaluation Criteria}

Actually, information visualization evaluation always presents difficulties and has been discussed frequently among the research community. Many works \cite{Plaisant2004, Lam2012} contribute to establishing a series of evaluation criteria and methods. The BELIV workshop series focuses on the challenges of evaluation in visualization. However, the evaluation targeting at narrative authoring tools faces another obstacle. For example, typical evaluation is based on controlled experiments to compare users' performance such as completion time and error rates. But it is challenging to assess these criteria in view of an authoring system which supports users' creative work. It is hard to calculate time and errors for a visual story creation. What's more, the primary problem is the lack of an appropriate comparative system. Most of novel authoring systems are designed and developed due to the insufficient functions of existing systems. It is reasonable that the newly-developed tool should outperform the existing one. However, users must have the diverse knowledge of tools. Like, a professional Adobe Illustrator designer gets accustomed to using this software but may not have enough time to learn the full capacities of the new authoring system, where the tutorial is mostly limited to a short study session. It brings difficulties to control essential factors and set tasks during the evaluation experiment. Therefore, a series of evaluation criteria is required to guide the approach.

Amini et al. \cite{Amini2018a} recently proposed a variety of criteria to consider when evaluating a narrative authoring tool. It is natural that the list does not cover an exhaustive set of criteria. It still sheds light on our thinking. We propose a tailored list of criteria combined with their work and evaluation standards derived from the previous survey of relevant tools. And with an increasing trend of narrative authoring tools in the future, the study of criteria for evaluating data-driven storytelling tools will grow at length. Currently, they can be grouped to eight aspects as follows. 

\begin{compactitem}
	\item \textbf{Expressiveness}.  The tool provides with a number of design choices, including various visualization types, annotation, and interactions. Users are allowed to create a range of possible visual data stories with the system.

\item \textbf{Flexibility}. It relates to the extent to which an author can extend the given library of design choices and make custom designs. This criterion demands a balance between creative work and ease of creation. 

\item \textbf{Guidance}. Some tools target at novice users who are likely to have problems when creating stories. It would be of great help to provide design support such as making layout suggestions during the process. 

\item \textbf{Efficiency}. It concerns the speed to produce a data-driven story using the tool, i.e., users can create how many stories within the limited time. This criterion is of great importance to some kinds of users such as journalists who need to produce stories frequently and quickly. 

\item \textbf{Usability}. Given an expected data-driven story, the tool should provide a series of functions for users to craft it.

\item \textbf{Learnability}. The learning curve of the storytelling tool are closely related to the number of functions and intuitive interactions the tool provides. Qualified documents and abundant tutorial materials can also help ease the learning. 

\item \textbf{Integration}. An authoring tool for narrative visualization is supposed to be integrated into current workflow and assist in authors' creative work. This criterion asks tool builders to understand the methods authors are using to create data-driven stories.

\item \textbf{Iteration}. The process to craft a data-driven story inevitably involves several iterations. It assesses the extent to which the tool supports iterative controls.

\end{compactitem}

Actually, a qualified narrative authoring tool is not necessarily required to satisfy all of these criteria. Good tool designers should strike a balance and make trade-offs among these criteria when they build such tools. Suitable evaluation standards should depend upon the research contribution that the system intends to achieve. For example, if the contribution of a tool is to help automatically construct annotated visualizations to contextualize data-rich texts (e.g., \cite{Hullman2013}), flexibility may be not adopted for the evaluation. But when the system aims to allow users to realize a broad range of designs (e.g., \cite{Wang2018}), the evaluation should attach great importance to the expressiveness criterion. 

\section{Evaluation Methods}

Ren et al. \cite{Ren2018} discussed the evaluation methods used in many visualization authoring systems whose main research contribution lies on “expressive” designs. They summarized six approaches to conduct evaluation in these works. We, here, mainly follow their methodology but just focus on storytelling tools as discussed in Section 4.1. Since general visualization authoring systems do not necessarily equip with the capability of storytelling, they mostly provide visualization for data exploration and analysis like Microsoft Excel. Authors need to edit the generated visualization with other tools to support narratives. Although some tools like DataInk \cite{Xia2018} focus on binding data with Information graphics, their main contribution lies on creative design not data-driven storytelling. Notably, we include InfoNice \cite{Wang2018}, because it is incorporated into an analytics tool, bridging the gap between data exploration and presentation. Another exception is SketchStory \cite{Lee2013}, which allows presenters to freeform sketch on a digital whiteboard  during the process. Therefore, we summarize the evaluation methods of narrative authoring tools in Table \ref{tab:authoring-tool}, which is different from the one of Ren et al. in some aspects. 

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		& Approaches  & \begin{tabular}[c]{@{}c@{}}Empirical\\ Study\end{tabular} & \begin{tabular}[c]{@{}c@{}}Reproduct\\-ion Study\end{tabular} & \begin{tabular}[c]{@{}c@{}}Free-Form\\ Study\end{tabular} & \begin{tabular}[c]{@{}c@{}}Comparative\\ Study\end{tabular} & Gallery    \\ \hline
		Contextifier\cite{Hullman2013}                                                   & automated   & survey                                                    &                                                              &                                                           & y                                                           &            \\ \hline
		NewsViews\cite{Gao2014}                                                      & automated   & survey                                                    &                                                              &                                                           &                                                             &            \\ \hline
		Sequence\cite{Hullman2013a}                                                       & automated   & survey                                                    &                                                              &                                                           &                                                             &            \\ \hline
		GraphScape\cite{Kim2017a}                                                     & automated   &                                                           &                                                              &                                                           &                                                             &            \\ \hline
		DataClips\cite{Amini2017}                                                      & authoring   & survey                                                    &                                                              & y                                                         & video design                                                &            \\ \hline
		ChartAccent\cite{Ren2017}                                                    & authoring   & survey                                                    & y                                                            &                                                           &                                                             &            \\ \hline
		\begin{tabular}[c]{@{}c@{}}Timeline\\ Storyteller\cite{storyteller}\end{tabular} & authoring   & survey                                                    &                                                              &                                                           &                                                             &            \\ \hline
		Ellipsis\cite{Satyanarayan2014}                                                       & authoring   & interviews                                                &                                                              & y                                                         &                                                             & y          \\ \hline
		InfoNice\cite{Wang2018}                                                       & authoring   &                                                           &                                                              &                                                           & y                                                           & y          \\ \hline
		SketchStory\cite{Lee2013}                                                    & authoring   & \begin{tabular}[c]{@{}c@{}}wizard-of\\ -oz\end{tabular}   & y                                                            & y                                                         & y                                                           &            \\ \hline
		Hanpuku\cite{Bigelow2017}                                                        & program &                                                           &                                                              &                                                           &                                                             & 3 cases \\ \hline
	\end{tabular}
	\caption{Evaluation methods used for each of the narrative authoring tools. "y" means the evaluation method was conducted for this tool. }
	\label{tab:authoring-tool}
\end{table}

\subsection{Empirical Study}
Brehmer et al. \cite{Brehmer2014} introduced empirical study in a broad way with a multitude of methods, including observational field studies, interviews, and so on. They encouraged pre-design empirical studies for information visualization. One obvious benefit was to characterize work practices and associated problems, thus informing the design space. In the data-driven storytelling field, many works present exemplary instances through a pre-design empirical study. A favorite way is to analyze a curated collection of existing stories. For example, Brehmer et al. first surveyed 263 timelines and identified 14 design choices with three dimensions \cite{Brehmer2017} to build Timeline Storyteller \cite{storyteller}. Amini et al. \cite{Amini2015} attained empirical knowledge with two exploratory studies. The first was to identify high-level structures with key components based on a qualitative examination of 50 online data videos through the cinematography lens. Then, they observed the creation process of 13 experienced storytellers. The derived design space worked as the cornerstone for DataClips \cite{Amini2017} they developed for authoring data videos. Another way to understand empirical knowledge is to involve end users. For example, Satyanarayan and Heer \cite{Satyanarayan2014} conducted formative interviews with experienced story makers to learn the general authoring process and limitations of current tools. SketchStory \cite{Lee2013} conducted a Wizard of Oz study and usage tests with participants \cite{Walny2012} to iterate the design.

In summary, when designing authoring tools, empirical studies play a critical role both in informing design spaces and justifying trade-offs. They provide an initial understanding about the tool framework. 

\subsection{Reproduction Study}
In a reproduction study, participants are asked to reproduce a copy of the given visual data stories. This study is often served as a tutorial to familiarize subjects with the authoring tool, which first examines its learnability. ChartAccent \cite{Ren2017} and SketchStory \cite{Lee2013} both featured a reproduction study. It should be noted that this study can encounter the usability issues. For example, participants complained that ChartAccent \cite{Ren2017} didn’t support z-order manipulation. SketchStory \cite{Lee2013} iterated the system design after the reproduction study and free-from study.

However, the main weakness is that a reproduction study does not conform to the reality. Such tools are built to support creative work and author novel data-driven narratives. The finding from a reproduction study can not totally demonstrate the usability.

\subsection{Free-Form Study}
A free-form study requires participants to author their own visual data stories with the tool. DataClips \cite{Amini2017}, Ellipsis \cite{Satyanarayan2014}, and SketchStory \cite{Lee2013} all utilized this evaluation. This study simulates the real-world usage scenario and can be used to assess multiple aspects of the authoring tool, such as expressiveness, learnability, and usability. In this experiment, it should be noted that there is likely to be a phase before creation for users to get familiar with data and facts \cite{Amini2017}, as well as generate and sketch ideas \cite{Lee2013}. 

However, a free-form study tends to last limited time duration. That means the participants may not have enough time to master all functions the tool provides, as well as create a variety of visualization artifacts. Therefore, the expressiveness can’t be demonstrated fully in this study.

\subsection{Comparative Study}
Tool builders are likely to conduct a controlled experiment to compare their tools with existing commercial software tools. It can be used to highlight the usability and efficiency. For example, Wang et al. compared InfoNice \cite{Wang2018} with Data-Driven Guides \cite{Kim2017} and measured time to create an infographic-style visualization with the same data using these two tools. Lee et al. \cite{Lee2013} employed SketchStory to do real-time presentations as compared to the traditional way using PowerPoint. They measured the engagement for both the audience and presenter and found sketching facilitate storytelling in that scenario. Amini et al. \cite{Amini2017} asked participants to create data video using DataClips against the combination of Adobe Illustrator and Adobe Effects, because there was no such similar tool to craft data videos directly. Thus, it brings a problem that the experiment is not under strict controls. It is hard to find an appropriate evaluation matrix to compare tools accurately. We have remarked that research prototypes contribute to novel features and may not incorporate some engineering functions such as “undo/redo”. However, in practice, these functions do work. Thus, it may decrease the impression among participants and also increase the creation time in the comparative study. 

\subsection{Gallery}
As we have mentioned in Section 4.3.3, subjects can only master limited capability to show expressiveness of the tool in limited time. Researchers recently have thought to provide a gallery themselves to demonstrate expressiveness. Ellipsis \cite{Satyanarayan2014}, InfoNice \cite{Wang2018}, and Hanpuku \cite{Bigelow2017} all offer galleries for review. The benefits are obvious in expressiveness and efficiency.  However, a major concern might be the target users can’t produce such visualization artifacts as the authors do. Detailed tutorials and instruments should be complemented. Since the gallery is provided by the tool builder, it can not show the usability and learnability of tool. It is supposed to be combined with other evaluation methods. 


 \newpage

